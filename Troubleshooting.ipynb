{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import timeit\n",
    "import pickle\n",
    "import numpy as np\n",
    "from pandas import DataFrame, Series\n",
    "import multiprocessing as mp\n",
    "from forest_surveyor import p_count, p_count_corrected\n",
    "import forest_surveyor.datasets as ds\n",
    "from forest_surveyor.structures import forest_walker, batch_getter, rule_tester, loo_encoder, rule_accumulator\n",
    "from forest_surveyor.routines import tune_rf, train_rf, evaluate_model, run_batch_explanations, anchors_preproc, anchors_explanation\n",
    "from scipy.stats import chi2_contingency\n",
    "from math import sqrt\n",
    "from sklearn.metrics import confusion_matrix, cohen_kappa_score, precision_recall_fscore_support, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mine_path_segments(walked, data_container,\n",
    "                        support_paths=0.1, alpha_paths=0.5,\n",
    "                        disc_path_bins=4, disc_path_eqcounts=False,\n",
    "                        which_trees='majority'):\n",
    "\n",
    "    # discretize any numeric features\n",
    "    walked.discretize_paths(data_container.var_dict,\n",
    "                            bins=disc_path_bins,\n",
    "                            equal_counts=disc_path_eqcounts)\n",
    "\n",
    "    # the patterns are found but not scored and sorted yet\n",
    "    walked.mine_patterns(support=support_paths)\n",
    "    print('done m')\n",
    "    return(walked)\n",
    "\n",
    "def score_sort_path_segments(walked, data_container,\n",
    "                                sample_instances, sample_labels, encoder,\n",
    "                                alpha_paths=0.5, weighting='chisq'):\n",
    "    # best at -1 < alpha < 1\n",
    "    # the patterns will be weighted by chi**2 for independence test, p-values\n",
    "    if weighting == 'chisq':\n",
    "        weights = [] * len(walked.patterns)\n",
    "        for wp in walked.patterns:\n",
    "            rt = rule_tester(data_container=data_container,\n",
    "                            rule=wp,\n",
    "                            sample_instances=sample_instances)\n",
    "            rt.sample_instances = encoder.transform(rt.sample_instances)\n",
    "            idx = rt.apply_rule()\n",
    "            covered = p_count_corrected(sample_labels[idx], [i for i in range(len(data_container.class_names))])['counts']\n",
    "            not_covered = p_count_corrected(sample_labels[~idx], [i for i in range(len(data_container.class_names))])['counts']\n",
    "            observed = np.array((covered, not_covered))\n",
    "\n",
    "            # this is the chisq based weighting. can add other options\n",
    "            if covered.sum() > 0 and not_covered.sum() > 0: # previous_counts.sum() == 0 is impossible\n",
    "                weights.append(sqrt(chi2_contingency(observed=observed[:, np.where(observed.sum(axis=0) != 0)], correction=True)[0]))\n",
    "            else:\n",
    "                weights.append(max(weights))\n",
    "\n",
    "        # now the patterns are scored and sorted. alpha > 0 favours longer patterns. 0 neutral. < 0 shorter.\n",
    "        walked.sort_patterns(alpha=alpha_paths, weights=weights) # with chi2 and support sorting\n",
    "    else:\n",
    "        walked.sort_patterns(alpha=alpha_paths) # with only support/alpha sorting\n",
    "    return(walked)\n",
    "\n",
    "def get_rule(rule_acc, encoder, sample_instances, sample_labels, pred_model,\n",
    "                        greedy='precision', precis_threshold=0.95):\n",
    "\n",
    "        # run the rule accumulator with greedy precis\n",
    "        rule_acc.build_rule(encoder=encoder,\n",
    "                    sample_instances=sample_instances,\n",
    "                    sample_labels=sample_labels,\n",
    "                    greedy=greedy,\n",
    "                    prediction_model=pred_model,\n",
    "                    precis_threshold=precis_threshold)\n",
    "        rule_acc.prune_rule()\n",
    "        ra_lite = rule_acc.lite_instance()\n",
    "\n",
    "        # collect completed rule accumulator\n",
    "        return(ra_lite)\n",
    "\n",
    "def as_chirps(walked, data_container,\n",
    "                        sample_instances, sample_labels, encoder, pred_model,\n",
    "                        support_paths=0.1, alpha_paths=0.5,\n",
    "                        disc_path_bins=4, disc_path_eqcounts=False,\n",
    "                        which_trees='majority', weighting='chisq',\n",
    "                        greedy='precis', precis_threshold=0.95,\n",
    "                        batch_idx=None):\n",
    "    # these steps make up the CHIRPS process:\n",
    "    # mine paths for freq patts\n",
    "    walked = mine_path_segments(walked, data_container,\n",
    "                            support_paths, alpha_paths,\n",
    "                            disc_path_bins, disc_path_eqcounts,\n",
    "                            which_trees)\n",
    "    # score and sort\n",
    "    walked = score_sort_path_segments(walked, data_container,\n",
    "                                    sample_instances, sample_labels,\n",
    "                                    encoder, alpha_paths, weighting)\n",
    "    # greedily add terms to create rule\n",
    "    ra = rule_accumulator(data_container=data_container, paths_container=walked)\n",
    "    ra_lite = get_rule(ra, encoder, sample_instances, sample_labels, pred_model,\n",
    "    greedy, precis_threshold)\n",
    "\n",
    "    return(batch_idx, ra_lite)\n",
    "\n",
    "\n",
    "def run_b(f_walker, getter,\n",
    " data_container, encoder, sample_instances, sample_labels,\n",
    " batch_size = 1, n_batches = 1,\n",
    " support_paths=0.1, alpha_paths=0.5,\n",
    " disc_path_bins=4, disc_path_eqcounts=False,\n",
    " alpha_scores=0.5, which_trees='majority',\n",
    " precis_threshold=0.95, weighting='chisq', greedy='greedy',\n",
    " forest_walk_async=False, chirps_explanation_async=False):\n",
    "\n",
    "    pred_model = f_walker.prediction_model\n",
    "    # create a list to collect completed rule accumulators\n",
    "    completed_rule_accs = [[]] * (batch_size * n_batches)\n",
    "\n",
    "    for b in range(n_batches):\n",
    "        print('walking forest for batch ' + str(b) + ' of batch size ' + str(batch_size))\n",
    "        instances, labels = getter.get_next(batch_size)\n",
    "        instance_ids = labels.index.tolist()\n",
    "        # get all the tree paths instance\n",
    "        batch_walked = f_walker.forest_walk(instances = instances\n",
    "                                , labels = labels\n",
    "                                , async = forest_walk_async)\n",
    "\n",
    "        for batch_idx in range(batch_size):\n",
    "            instance_id = instance_ids[batch_idx]\n",
    "            # extract the current instance paths for freq patt mining, filter by majority trees only\n",
    "            walked = batch_walked.get_instance_paths(batch_idx, which_trees=which_trees)\n",
    "            walked.instance_id = instance_id\n",
    "            print(batch_idx, instance_id)\n",
    "            walked = mine_path_segments(walked, data_container, \n",
    "                        support_paths, alpha_paths,\n",
    "                        disc_path_bins, disc_path_eqcounts,\n",
    "                        which_trees)\n",
    "            walked = score_sort_path_segments(walked, data_container,\n",
    "                                sample_instances, sample_labels, encoder,\n",
    "                                alpha_paths, weighting)\n",
    "            \n",
    "            ra = rule_accumulator(data_container=data_container, paths_container=walked)\n",
    "            ra_lite = get_rule(ra, encoder, sample_instances, sample_labels, pred_model,\n",
    "                                greedy, precis_threshold)\n",
    "\n",
    "            # run the chirps process on each instance paths\n",
    "#             _, completed_rule_acc = \\\n",
    "#                 as_chirps(walked, data_container,\n",
    "#                 sample_instances, sample_labels,\n",
    "#                 encoder, pred_model,\n",
    "#                 support_paths, alpha_paths,\n",
    "#                 disc_path_bins, disc_path_eqcounts,\n",
    "#                 which_trees, weighting,\n",
    "#                 greedy, precis_threshold,\n",
    "#                 batch_idx)\n",
    "\n",
    "#             # add the finished rule accumulator to the results\n",
    "#             completed_rule_accs[b * batch_size + batch_idx] = [completed_rule_acc]\n",
    "    return(instances, labels, instance_ids, batch_walked, walked)\n",
    "    #return(batch_walked, walked, ra, ra_lite, batch_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using previous tuning parameters\n",
      "walking forest for batch 0 of batch size 3\n",
      "0 399\n",
      "done m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\id126493\\Documents\\GitHub\\interpret_basics2\\forest_surveyor\\structures.py:259: RuntimeWarning: invalid value encountered in true_divide\n",
      "  np.histogram(lowers, lower_bins)[0]).round(5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 250\n",
      "done m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\id126493\\Documents\\GitHub\\interpret_basics2\\forest_surveyor\\structures.py:255: RuntimeWarning: invalid value encountered in true_divide\n",
      "  np.histogram(uppers, upper_bins)[0]).round(5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 396\n",
      "done m\n"
     ]
    }
   ],
   "source": [
    "random_state = 125\n",
    "override_tuning=False\n",
    "add_trees=0\n",
    "n_instances=3\n",
    "n_batches=1\n",
    "\n",
    "mydata = ds.credit_data(random_state=random_state)\n",
    "tt = mydata.tt_split(random_state=123)\n",
    "\n",
    "best_params = tune_rf(tt['X_train_enc'], tt['y_train'],\n",
    " save_path = mydata.pickle_path(),\n",
    " random_state=mydata.random_state,\n",
    " override_tuning=override_tuning)\n",
    "\n",
    "best_params['n_estimators'] = best_params['n_estimators'] + add_trees\n",
    "\n",
    "# train a rf model\n",
    "rf, enc_rf = train_rf(X=tt['X_train_enc'], y=tt['y_train'],\n",
    " best_params=best_params,\n",
    " encoder=tt['encoder'],\n",
    " random_state=mydata.random_state)\n",
    "\n",
    "# fit the forest_walker\n",
    "f_walker = forest_walker(forest = rf,\n",
    " data_container=mydata,\n",
    " encoder=tt['encoder'],\n",
    " prediction_model=enc_rf)\n",
    "\n",
    "# run the batch based forest walker\n",
    "getter = batch_getter(instances=tt['X_test'], labels=tt['y_test'])\n",
    "\n",
    "# faster to do one batch, avoids the overhead of setting up many but consumes more mem\n",
    "# get a round number of instances no more than what's available in the test set\n",
    "n_instances = min(n_instances, len(tt['y_test']))\n",
    "batch_size = int(n_instances / n_batches)\n",
    "n_instances = batch_size * n_batches\n",
    "\n",
    "# collect completed rule_acc_lite objects for the whole batch\n",
    "#batch_walked, walked, ra, ra_lite, batch_idx = run_b(f_walker=f_walker,\n",
    "instances, labels, instance_ids, batch_walked, walked = run_b(f_walker=f_walker,                                                     \n",
    " getter=getter,\n",
    " data_container=mydata,\n",
    " encoder=tt['encoder'],\n",
    " sample_instances=tt['X_train'],\n",
    " sample_labels=tt['y_train'],\n",
    " support_paths=0.05,\n",
    " alpha_paths=0.5,\n",
    " disc_path_bins=4,\n",
    " disc_path_eqcounts=False,\n",
    " alpha_scores=0.5,\n",
    " which_trees='majority',\n",
    " precis_threshold=0.95,\n",
    " batch_size=batch_size,\n",
    " n_batches=n_batches,\n",
    " weighting='chisq',\n",
    " greedy='precis',\n",
    " forest_walk_async=False,\n",
    " chirps_explanation_async=False)\n",
    "\n",
    "batch_idx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instance_id': 399,\n",
       " 'path': {'feature_idx': [],\n",
       "  'feature_name': [],\n",
       "  'feature_value': [],\n",
       "  'leq_threshold': [],\n",
       "  'threshold': []},\n",
       " 'pred_class': 0,\n",
       " 'pred_class_label': 'minus',\n",
       " 'pred_proba': [0.5445134575569358, 0.4554865424430642],\n",
       " 'tree_correct': True,\n",
       " 'true_class': 0}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_walked.path_detail[648][batch_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_walked = f_walker.forest_walk(instances = instances\n",
    "                        , labels = labels\n",
    "                        , async = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0.])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instances = tt['encoder'].transform(instances)\n",
    "if 'todense' in dir(instances): # it's a sparse matrix\n",
    "    instances = instances.todense()\n",
    "n_features = instances.shape[1]\n",
    "\n",
    "f_walker.forest.estimators_[648].predict(instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2], dtype=int64)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# f_walker.forest.estimators_[648].tree_.decision_path()\n",
    "f_walker.forest.estimators_[648].tree_.feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\id126493\\Documents\\GitHub\\interpret_basics2\\forest_surveyor\\structures.py:259: RuntimeWarning: invalid value encountered in true_divide\n",
      "  np.histogram(lowers, lower_bins)[0]).round(5)\n"
     ]
    }
   ],
   "source": [
    "instance_id = instance_ids[2]\n",
    "# extract the current instance paths for freq patt mining, filter by majority trees only\n",
    "walked = batch_walked.get_instance_paths(batch_idx, which_trees='majority')\n",
    "walked.instance_id = instance_id\n",
    "\n",
    "walked = mine_path_segments(walked, mydata, \n",
    "                            support_paths=0.05, alpha_paths=0.5,\n",
    "                            disc_path_bins=4, disc_path_eqcounts=False,\n",
    "                            which_trees='majority')\n",
    "\n",
    "walked = score_sort_path_segments(walked, mydata,\n",
    "                    encoder=tt['encoder'],\n",
    " sample_instances=tt['X_train'],\n",
    " sample_labels=tt['y_train'],\n",
    "                    alpha_paths=0.5, weighting='chisq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A14', False, 85.14286),\n",
       " ('A10_t', True, 0.5),\n",
       " ('A9_f', False, 0.5),\n",
       " ('A7_v', True, 0.5),\n",
       " ('A13_g', False, 0.5),\n",
       " ('A2', False, 21.54912)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "walked.paths[648]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[39.66999816894531, 48.125, 36.459999084472656, 55.665000915527344, 58.0, 36.459999084472656, 36.459999084472656, 36.625, 54.375, 52.665000915527344, 29.915000915527344, 35.209999084472656, 36.459999084472656, 35.0, 51.084999084472656, 40.959999084472656, 34.875, 52.459999084472656, 48.125, 36.625, 41.04499816894531, 45.5, 40.959999084472656, 46.70500183105469, 48.290000915527344, 29.915000915527344, 31.78408432006836, 41.04499816894531, 32.375, 35.790000915527344, 36.459999084472656, 43.084999084472656, 34.66999816894531, 30.329999923706055, 31.78408432006836, 40.959999084472656, 36.459999084472656, 33.084999084472656, 34.290000915527344, 32.040000915527344, 45.58000183105469, 51.040000915527344, 52.209999084472656, 41.25, 37.415000915527344, 33.625, 41.25, 36.459999084472656, 41.04499816894531, 53.33000183105469, 34.415000915527344, 47.709999084472656, 63.290000915527344, 50.834999084472656, 37.625, 34.959999084472656, 41.25, 34.875, 34.875, 29.920000076293945, 46.125, 33.375, 42.16999816894531, 35.5, 35.084999084472656, 40.959999084472656, 46.75, 36.665000915527344, 32.040000915527344, 46.75, 38.54499816894531, 36.709999084472656, 33.25, 40.75, 63.290000915527344, 53.040000915527344, 45.5, 45.58000183105469, 36.625, 33.209999084472656, 52.665000915527344, 33.375, 36.459999084472656, 48.125, 36.459999084472656, 48.125, 43.0, 34.25, 40.375, 46.70500183105469, 35.790000915527344, 52.459999084472656, 37.375, 33.459999084472656, 52.415000915527344, 34.875, 39.75, 33.209999084472656, 40.875, 53.209999084472656, 34.959999084472656, 63.290000915527344, 46.290000915527344, 31.044998168945312, 48.125, 33.459999084472656, 36.459999084472656, 47.625, 52.459999084472656, 34.415000915527344, 32.125, 53.459999084472656, 57.20500183105469] [25.625, 23.75, 24.0, 23.334999084472656, 18.084999084472656, 19.295000076293945, 20.25, 16.625, 21.0, 18.125, 23.334999084472656, 19.334999084472656, 22.165000915527344, 22.165000915527344, 22.040000915527344, 24.0, 18.084999084472656, 24.0, 24.209999084472656, 20.375, 20.25, 25.209999084472656, 25.834999084472656, 18.125, 16.25, 22.165000915527344, 24.0, 21.959999084472656, 20.25, 20.25, 21.665000915527344, 24.545000076293945, 20.415000915527344, 19.204999923706055, 20.25, 22.125, 20.25, 22.709999084472656, 24.209999084472656, 22.795000076293945, 19.295000076293945, 19.375, 25.790000915527344, 22.709999084472656, 27.625, 26.625, 25.625, 25.084999084472656, 22.795000076293945, 27.915000915527344, 18.165000915527344, 19.334999084472656, 18.5, 18.084999084472656, 20.75, 18.125, 24.415000915527344, 18.165000915527344, 19.795000076293945, 24.0, 22.459999084472656, 18.334999084472656, 19.334999084472656, 22.125, 22.125, 22.790000915527344, 23.75, 28.5, 19.5, 25.625, 26.459999084472656, 18.165000915527344, 19.295000076293945, 22.795000076293945, 19.334999084472656, 20.75, 24.209999084472656, 19.334999084472656, 18.875, 19.295000076293945, 22.125, 24.959999084472656, 18.125, 25.875, 24.0, 24.625, 24.125, 22.709999084472656, 16.415000915527344, 17.5, 26.584999084472656, 20.25, 24.125, 26.084999084472656, 25.875, 18.125, 22.709999084472656, 20.625, 20.375, 27.584999084472656, 17.959999084472656, 24.0, 18.084999084472656, 27.084999084472656, 27.584999084472656, 18.084999084472656, 18.665000915527344, 22.579999923706055, 18.084999084472656, 18.125, 27.329999923706055, 24.0, 23.375, 24.0, 18.125, 25.875, 22.125, 18.375, 20.25, 25.959999084472656, 18.084999084472656, 22.5, 19.334999084472656, 21.0, 24.540000915527344, 23.375, 29.125, 23.834999084472656, 24.540000915527344, 29.790000915527344, 20.25, 22.709999084472656, 19.625, 25.704999923706055, 25.704999923706055, 18.084999084472656, 24.170000076293945, 18.125, 22.125, 16.25, 25.165000915527344, 16.625, 19.454999923706055, 23.295000076293945, 22.795000076293945, 24.0, 22.165000915527344, 18.125, 22.125, 22.875, 21.084999084472656, 22.415000915527344, 24.415000915527344, 20.329999923706055, 26.665000915527344, 24.209999084472656, 18.125, 24.0, 25.625, 24.875, 18.125, 19.334999084472656, 20.415000915527344, 24.209999084472656, 26.625, 18.084999084472656, 28.5, 22.875, 22.125, 25.415000915527344, 22.125, 20.0, 21.209999084472656, 18.704999923706055, 21.0, 23.045000076293945, 20.25]\n",
      "[5.019999980926514, 7.375, 5.917500019073486, 9.5, 3.6875, 4.167500019073486, 8.354999542236328, 4.167500019073486, 3.2074999809265137, 2.7300000190734863, 8.542499542236328, 8.542499542236328, 3.625, 4.605000019073486, 4.332499980926514, 4.332499980926514, 4.332499980926514, 9.125, 3.6875, 13.25, 2.6449999809265137, 4.1875, 4.019999980926514, 3.125, 4.019999980926514, 3.625, 2.0824999809265137, 5.1875, 4.332499980926514, 5.292500019073486, 4.269999980926514, 4.1875, 4.019999980926514, 4.605000019073486, 4.019999980926514, 4.3125, 4.3125, 4.5625, 4.269999980926514, 3.3125, 9.270000457763672, 4.019999980926514, 4.1875, 2.125, 4.332499980926514, 4.019999980926514, 4.332499980926514, 2.7300000190734863, 4.019999980926514, 4.605000019073486, 3.3125, 13.417499542236328, 6.75, 5.25, 4.269999980926514, 8.542499542236328, 9.125, 9.75, 3.8324999809265137, 5.019999980926514, 4.355000019073486, 4.355000019073486, 4.605000019073486, 4.332499980926514, 3.6875, 3.8949999809265137, 11.164999961853027, 4.019999980926514, 4.019999980926514, 4.019999980926514, 12.414999961853027, 3.8949999809265137, 2.1024999618530273, 4.1875, 4.019999980926514, 9.979999542236328, 5.0625, 7.375, 4.375, 4.355000019073486, 5.019999980926514, 9.270000457763672, 3.3949999809265137, 5.582499980926514, 4.019999980926514, 11.3125, 6.042500019073486, 4.792500019073486, 9.270000457763672, 4.1875, 4.375, 4.332499980926514, 3.3125, 4.019999980926514, 5.019999980926514, 3.3125, 6.0625, 5.5625, 4.75, 3.8949999809265137, 2.1024999618530273, 8.75, 7.375, 3.6875, 3.8949999809265137] [0.6875, 0.20750001072883606, 0.20750001072883606, 0.20749999582767487, 0.14500001072883606, 0.39499998092651367, 1.0399999618530273, 0.26999998092651367, 0.0625, 0.6850000023841858, 0.0625, 0.39499998092651367, 0.6875, 0.16750000417232513, 0.14500001072883606, 0.1875, 0.0625, 0.16750000417232513, 0.8324999809265137, 0.10250000655651093, 0.39499998092651367, 0.375, 0.8324999809265137, 0.019999999552965164, 0.27250000834465027, 0.14500001072883606, 0.39499998092651367, 0.1850000023841858, 0.39499998092651367, 0.8324999809265137, 0.8324999809265137, 0.08249999582767487, 0.14500001072883606, 0.1850000023841858, 0.0625, 0.6850000023841858, 0.16750000417232513, 0.26999998092651367, 0.16750000417232513, 0.39499998092651367, 0.8324999809265137, 0.16750000417232513, 0.1875, 0.1850000023841858, 0.5425000190734863, 0.0625, 0.8324999809265137, 0.08249999582767487, 0.6875, 0.1850000023841858, 1.0824999809265137, 0.75, 0.1850000023841858, 0.39499998092651367, 0.22999998927116394, 0.75, 0.16750000417232513, 0.3974999785423279, 0.8350000381469727, 0.39499998092651367, 1.7300000190734863, 0.26999998092651367, 0.8324999809265137, 0.16750000417232513, 0.3974999785423279, 0.16750000417232513, 0.3100000023841858, 0.35249999165534973, 0.10500000417232513, 0.019999999552965164, 0.16750000417232513, 0.22999998927116394, 0.9149999618530273, 0.16750000417232513, 0.1850000023841858, 0.14500001072883606, 0.39499998092651367, 0.35499998927116394, 0.16750000417232513, 0.16750000417232513, 0.8550000190734863, 0.39499998092651367, 0.6675000190734863, 0.1850000023841858, 0.39499998092651367, 0.16499999165534973, 0.16750000417232513, 0.20750001072883606, 0.6875, 0.14500001072883606, 0.20749999582767487, 0.16750000417232513, 0.8324999809265137, 0.1875, 0.47999998927116394, 0.39499998092651367, 0.1850000023841858, 0.375, 0.14500001072883606, 0.08249999582767487, 0.1875, 0.7074999809265137, 0.16750000417232513, 0.8550000190734863, 0.35499998927116394, 0.16750000417232513, 0.7275000214576721, 0.14750000834465027, 0.7074999809265137, 0.7074999809265137, 0.39499998092651367, 0.8324999809265137, 0.39499998092651367, 0.39499998092651367, 0.8324999809265137, 0.22999998927116394, 0.20749999582767487, 0.14750000834465027, 0.16750000417232513, 0.8324999809265137, 0.0625, 0.7074999809265137, 0.39499998092651367, 0.7074999809265137, 0.14750000834465027, 0.8324999809265137, 0.39499998092651367, 0.16750000417232513, 0.26999998092651367, 0.14500001072883606, 0.8550000190734863, 0.16750000417232513, 0.08249999582767487, 0.08249999582767487, 0.35249999165534973, 0.0625, 0.16750000417232513, 0.8324999809265137, 0.14500001072883606, 0.16750000417232513, 0.16750000417232513, 0.8324999809265137, 0.5425000190734863, 0.1850000023841858, 0.25, 0.14750000834465027, 1.9175000190734863, 0.20750001072883606, 0.3974999785423279, 0.20749999582767487, 0.20750001072883606, 0.39499998092651367, 0.375, 0.16750000417232513, 0.14750000834465027, 0.16750000417232513, 0.8324999809265137, 0.16750000417232513, 0.0625, 0.16750000417232513, 0.7074999809265137, 0.20750001072883606, 0.1875, 0.8324999809265137, 0.47749999165534973, 0.14500001072883606, 0.8524999618530273, 0.14500001072883606, 0.6675000190734863, 0.1875, 0.16750000417232513, 0.8324999809265137, 0.1850000023841858, 0.14500001072883606, 0.3974999785423279, 0.22749999165534973, 0.7074999809265137, 0.1850000023841858, 0.39499998092651367, 0.10500000417232513, 0.10250000655651093, 1.8949999809265137, 1.875, 0.16750000417232513, 0.08249999582767487, 0.14500001072883606, 0.41750001907348633, 0.08249999582767487, 0.3974999785423279, 0.3974999785423279, 0.0625, 0.7074999809265137, 0.1875, 0.3974999785423279, 0.8324999809265137, 0.16750000417232513, 0.20750001072883606, 0.14500001072883606, 0.8324999809265137, 1.875, 0.9375, 0.39499998092651367, 0.8524999618530273, 0.39499998092651367, 0.5625, 0.7074999809265137, 0.16750000417232513, 0.28999999165534973, 0.16750000417232513, 0.1875, 0.6875, 1.1449999809265137, 0.39499998092651367, 0.8324999809265137, 0.7074999809265137, 0.25, 0.019999999552965164, 0.1850000023841858, 0.1850000023841858, 0.6850000023841858, 0.3125, 0.16750000417232513, 0.8324999809265137, 0.16750000417232513, 0.16750000417232513, 0.6850000023841858, 0.39499998092651367, 0.8550000190734863, 0.20750001072883606, 0.8524999618530273, 0.1875, 0.8324999809265137, 0.375, 0.08249999582767487, 0.41750001907348633, 0.16750000417232513, 0.8324999809265137, 0.5824999809265137, 0.29250001907348633, 0.1875, 0.1875, 0.29250001907348633, 0.6875, 0.1850000023841858, 0.1850000023841858, 0.0625, 0.7074999809265137, 0.22999998927116394, 0.14500001072883606, 0.39499998092651367, 0.16750000417232513, 0.1850000023841858, 0.39499998092651367, 0.26999998092651367, 0.10500000417232513, 0.20749999582767487, 0.39499998092651367, 0.29250001907348633, 0.5425000190734863, 0.8324999809265137, 0.7074999809265137, 0.14500001072883606, 0.39499998092651367, 0.14500001072883606, 0.5399999618530273, 0.14500001072883606, 0.0625, 0.14500001072883606, 0.39499998092651367, 0.39499998092651367, 1.7300000190734863, 0.042500000447034836, 0.6850000023841858, 0.16750000417232513, 0.0625, 0.39499998092651367, 1.7699999809265137, 0.39499998092651367, 0.39499998092651367, 0.019999999552965164, 0.0625, 1.3125, 0.0625, 0.48000001907348633, 0.20749999582767487, 0.14500001072883606, 0.5425000190734863, 0.39499998092651367, 0.16750000417232513, 0.0625, 0.8324999809265137, 0.3974999785423279, 0.8324999809265137, 0.16750000417232513, 0.0625, 0.6875, 1.125, 0.08249999582767487, 0.1850000023841858, 0.14500001072883606, 0.39499998092651367, 0.10500000417232513, 0.26999998092651367, 0.14500001072883606, 0.9175000190734863, 0.10500000417232513, 0.7275000214576721, 0.16750000417232513, 0.1850000023841858, 1.059999942779541, 0.16750000417232513, 0.9175000190734863, 0.39499998092651367, 0.14500001072883606, 0.8324999809265137, 0.6850000023841858, 0.39499998092651367, 0.8324999809265137, 0.14500001072883606, 0.14500001072883606, 0.08249999582767487, 0.41749998927116394, 0.22999998927116394, 0.14500001072883606, 0.1850000023841858, 0.27000001072883606, 0.14500001072883606, 0.75, 0.1850000023841858, 0.8324999809265137, 0.39499998092651367, 0.22999998927116394, 0.7074999809265137, 1.0399999618530273, 0.3974999785423279, 0.0625, 1.375, 0.1875, 0.4599999785423279, 0.7074999809265137, 0.3974999785423279, 0.1875, 0.20750001072883606, 0.16750000417232513, 0.7074999809265137, 0.16750000417232513, 0.08249999582767487, 0.8324999809265137, 0.16499999165534973, 0.08249999582767487, 0.0625, 0.0625, 0.3125, 0.16750000417232513, 0.25, 0.8524999618530273, 0.0625, 0.8324999809265137, 1.5625, 0.8125, 0.08249999582767487, 0.39499998092651367]\n",
      "[1.3125, 0.375, 1.2699999809265137, 0.8149999976158142, 5.375, 1.0425000190734863, 2.625, 1.375, 1.375, 1.0199999809265137, 5.042500019073486, 0.6025000214576721, 5.0625, 3.8550000190734863, 5.625, 1.6050000190734863, 0.39499998092651367, 0.26999998092651367, 2.125, 1.6875, 0.0625, 1.8324999809265137, 1.5625, 0.22999998927116394, 1.625, 0.10500000417232513, 1.375, 1.0425000190734863, 0.35500001907348633, 5.25, 0.1875, 1.2074999809265137, 1.1875, 1.6875, 3.625, 5.0625, 5.125, 0.0625, 1.3125, 0.1875, 1.0199999809265137, 1.6475000381469727, 0.75, 0.33500000834465027, 1.375, 1.6050000190734863, 0.6050000190734863, 1.2074999809265137, 0.22999998927116394, 0.0625, 1.2699999809265137, 5.125, 1.6875, 5.625, 1.0199999809265137, 0.22999998927116394, 4.539999961853027, 0.14500001072883606, 2.125, 1.1875, 1.3324999809265137, 0.0625, 4.082499980926514, 2.0824999809265137, 2.125, 0.4375, 1.0425000190734863, 0.22999998927116394, 0.35500001907348633, 1.7074999809265137, 0.22999998927116394, 5.375, 5.292500019073486, 1.1875, 1.0425000190734863, 0.20750001072883606, 2.2074999809265137, 0.39499998092651367, 1.375, 3.9574999809265137, 0.8149999976158142, 1.375, 0.22999998927116394, 0.75, 5.292500019073486, 1.6875, 0.1875, 0.14500001072883606, 5.625, 0.0625, 5.625, 1.7074999809265137, 0.14500001072883606, 1.2074999809265137, 0.45750001072883606, 0.39499998092651367, 5.625, 5.417500019073486, 3.25, 1.375, 0.0625, 0.7925000190734863, 1.6050000190734863, 0.33249998092651367, 0.5199999809265137, 1.2699999809265137, 0.22999998927116394, 5.375, 1.3125, 0.4375, 0.0625, 3.4800000190734863, 5.125, 0.4375, 0.1875, 2.0824999809265137, 0.08249999582767487, 4.082499980926514, 4.539999961853027, 0.22999998927116394, 1.2699999809265137, 0.0625, 2.125, 1.6050000190734863, 1.0199999809265137, 0.0625, 0.5824999809265137, 0.14500001072883606, 1.375, 0.0625, 4.5625, 1.7074999809265137, 4.75, 0.14500001072883606, 6.75, 1.1875, 0.35500001907348633, 1.0425000190734863, 5.625, 0.7074999809265137, 0.22999998927116394, 6.0, 0.0625, 1.2699999809265137, 5.625, 5.0625, 3.0425000190734863, 1.375, 1.1875, 1.7074999809265137, 2.75, 3.625, 1.0199999809265137, 0.6025000214576721, 1.3125, 0.1875, 2.1675000190734863, 0.8125, 2.75, 0.7925000190734863, 1.8324999809265137, 0.0625, 5.625, 1.25, 1.0199999809265137, 4.539999961853027, 1.2699999809265137, 1.0425000190734863, 0.0625, 0.1875, 1.0199999809265137, 0.26999998092651367, 5.0625, 0.0625, 0.625, 0.0625, 0.0625, 1.0199999809265137, 2.125, 5.125, 4.539999961853027, 0.875, 1.6875, 2.125, 0.14500001072883606, 0.0625, 0.0625, 1.0199999809265137, 0.042500000447034836, 5.0625, 1.3324999809265137, 4.125, 1.2074999809265137, 0.22999998927116394, 1.0199999809265137, 1.2699999809265137, 1.0425000190734863, 5.625, 0.14500001072883606, 1.2699999809265137, 0.042500000447034836, 1.25, 1.7074999809265137, 1.125, 0.375, 1.6050000190734863, 1.0199999809265137, 1.2925000190734863, 5.625, 1.875, 0.8149999976158142, 0.0625, 3.7300000190734863, 0.75, 0.22999998927116394, 1.2699999809265137, 0.39499998092651367, 0.75, 0.0625, 0.14500001072883606, 0.39499998092651367, 1.375, 0.0625, 4.625, 5.625, 1.3125, 0.7925000190734863, 0.48000001907348633, 0.33249998092651367, 0.6050000190734863, 0.35500001907348633, 2.8949999809265137, 0.5824999809265137, 1.1875, 1.6050000190734863, 5.292500019073486, 1.8324999809265137, 1.2699999809265137, 5.625, 0.0625, 5.0625, 1.7074999809265137, 2.75, 0.22999998927116394, 5.625, 5.25, 1.375, 5.625, 0.39499998092651367, 1.5199999809265137, 0.0625, 2.0824999809265137, 1.6449999809265137, 1.1475000381469727, 0.0625, 1.4175000190734863, 5.125, 1.2699999809265137, 5.292500019073486, 2.0625, 0.5625, 1.3125, 0.22999998927116394, 0.22999998927116394, 1.1875, 1.2925000190734863, 0.6050000190734863, 1.7074999809265137, 5.0625, 0.8550000190734863, 0.5425000190734863, 1.6050000190734863, 0.14500001072883606, 1.2699999809265137, 0.26999998092651367, 1.6050000190734863, 5.125, 0.6875, 0.0625, 1.9149999618530273, 4.375, 2.125, 0.39499998092651367, 0.26999998092651367, 2.4574999809265137, 0.0625, 1.2699999809265137, 0.7925000190734863, 0.35500001907348633, 1.0425000190734863, 0.35500001907348633, 1.2074999809265137, 0.25, 0.14500001072883606, 0.6050000190734863, 1.6675000190734863, 1.2699999809265137, 0.20750001072883606, 0.14500001072883606, 0.26999998092651367, 1.2925000190734863, 1.2699999809265137, 2.125, 0.0625, 1.8324999809265137, 1.2074999809265137, 0.22999998927116394, 1.1875, 1.0199999809265137, 5.25, 0.45750001072883606, 1.0199999809265137, 0.39499998092651367, 1.6875, 1.375, 5.625, 0.0625, 2.0824999809265137, 1.2699999809265137, 0.0625, 1.375, 1.1875, 1.2925000190734863, 0.20750001072883606, 1.6875, 1.1875, 5.25, 1.2699999809265137, 1.1675000190734863, 1.2699999809265137, 1.25, 1.2699999809265137, 5.125, 1.2074999809265137, 1.375, 1.1875, 1.2074999809265137, 1.375, 0.1875, 1.0199999809265137, 1.2074999809265137, 0.0625, 1.1875, 4.75, 5.625, 2.125, 1.0199999809265137, 1.8324999809265137, 1.6875, 2.0824999809265137, 0.0625, 0.45750001072883606, 5.0625, 0.0625, 4.75, 1.0425000190734863, 1.2699999809265137, 0.0625, 0.22999998927116394, 1.2699999809265137, 0.0625, 0.22999998927116394, 0.5199999809265137, 1.625, 1.7699999809265137, 0.6050000190734863, 1.0199999809265137, 2.125, 0.9375, 5.417500019073486, 1.7699999809265137, 1.2699999809265137, 5.625, 1.1875, 0.0625, 0.1875, 4.8125, 0.7925000190734863, 0.14500001072883606, 1.7074999809265137, 0.4375, 5.125, 0.0625, 0.9175000190734863, 0.4375, 1.3125, 0.0625, 0.5625, 0.0625, 1.0425000190734863, 2.75, 1.2074999809265137, 3.0425000190734863, 5.25, 3.8550000190734863, 1.375, 1.0199999809265137, 0.20750001072883606, 1.375, 1.375, 1.1875, 1.0824999809265137, 0.1875, 4.75, 1.2699999809265137, 0.22999998927116394, 0.22999998927116394, 1.2074999809265137, 0.48000001907348633, 0.875, 0.0625, 0.1875, 5.625, 0.22999998927116394, 2.0824999809265137] [0.019999999552965164, 0.019999999552965164, 0.019999999552965164, 0.019999999552965164, 0.019999999552965164, 0.019999999552965164, 0.019999999552965164, 0.019999999552965164, 0.019999999552965164, 0.019999999552965164, 0.019999999552965164, 0.019999999552965164, 0.019999999552965164, 0.019999999552965164, 0.019999999552965164, 0.019999999552965164, 0.019999999552965164, 0.019999999552965164, 0.019999999552965164, 0.019999999552965164, 0.019999999552965164, 0.019999999552965164, 0.019999999552965164, 0.019999999552965164, 0.019999999552965164, 0.019999999552965164, 0.019999999552965164, 0.019999999552965164, 0.019999999552965164, 0.019999999552965164, 0.019999999552965164, 0.019999999552965164, 0.019999999552965164, 0.019999999552965164]\n",
      "[5.5, 3.5, 1.5, 0.5, 3.5, 2.5, 2.5, 0.5, 2.5, 2.5, 2.5, 0.5, 2.5, 1.5, 2.5, 2.5, 1.5, 6.5, 2.5, 0.5, 2.5, 0.5, 2.5, 0.5, 3.5, 2.5, 0.5, 2.5, 1.5, 2.5, 1.5, 0.5, 2.5, 0.5, 1.5, 3.5, 2.5, 2.5, 0.5, 1.5, 0.5, 2.5, 2.5, 2.5, 2.5, 2.5, 2.5, 3.5, 1.5, 3.5, 0.5, 2.5, 0.5, 2.5, 2.5, 1.5, 2.5, 0.5, 2.5, 2.5, 2.5, 3.5, 2.5, 2.5, 2.5, 2.5, 0.5, 3.5, 3.5, 2.5, 2.5, 3.5, 2.5, 0.5, 1.5, 0.5, 2.5, 2.5, 2.5, 3.5, 2.5, 2.5, 2.5, 2.5, 2.5, 1.5, 1.5, 0.5, 2.5, 0.5, 3.5, 2.5, 0.5, 2.5, 0.5, 2.5, 0.5, 0.5, 0.5, 2.5, 2.5, 0.5, 2.5, 2.5, 0.5, 2.5, 2.5, 3.5, 2.5, 2.5, 3.5, 2.5, 2.5, 1.5, 1.5, 2.5, 2.5, 3.5, 2.5, 2.5, 2.5, 2.5, 2.5, 0.5, 2.5, 3.5, 0.5, 2.5, 3.5, 0.5, 2.5, 2.5, 2.5, 2.5, 0.5, 1.5, 3.5, 0.5, 3.5, 2.5, 2.5, 0.5, 2.5, 2.5, 4.5, 0.5, 2.5, 2.5, 2.5, 2.5, 2.5, 0.5, 2.5, 0.5, 3.5, 2.5, 0.5, 0.5, 0.5, 2.5, 2.5, 1.5, 2.5, 0.5, 2.5, 2.5, 0.5, 2.5, 0.5, 0.5, 3.5, 0.5, 1.5, 2.5, 3.5, 0.5, 2.5, 1.5, 0.5, 2.5, 2.5, 2.5, 2.5, 0.5] []\n",
      "[192.00738525390625, 384.5, 294.5, 222.5, 142.5, 490.0, 224.0, 348.5, 182.00738525390625, 520.0, 296.0, 290.0, 182.00738525390625, 490.0, 472.5, 290.0, 223.0, 224.0, 182.00738525390625, 226.0, 364.0, 222.5, 223.0, 470.0, 450.0, 302.0, 232.5, 226.0, 490.0, 240.0, 430.0, 216.0, 290.0, 482.5, 182.00738525390625, 472.5, 515.0, 187.0, 345.0, 178.0, 224.0, 345.0, 223.0, 164.0, 182.00738525390625, 482.5, 182.00738525390625, 470.0, 472.5, 390.0, 224.0, 128.5, 290.0, 480.0, 129.5, 290.0, 157.0, 369.5, 364.5, 224.0, 224.0, 482.5, 378.0, 296.0, 150.0, 396.5, 129.5, 181.00738525390625, 231.5, 490.0, 304.5, 130.0, 490.0, 358.0, 420.0, 168.0, 290.0, 157.0, 438.0, 130.0, 555.0, 330.0, 290.0, 490.0, 437.0, 467.0, 368.0, 490.0, 205.5, 182.00738525390625, 129.0, 159.0, 330.0, 296.0, 296.0, 264.0, 350.0, 216.0, 192.00738525390625, 511.5, 330.0, 224.0, 472.5, 290.0, 149.0, 180.0, 470.0, 250.0, 130.0, 157.0, 290.0, 290.0, 231.5, 128.5, 350.0, 472.5, 290.0, 157.0, 330.0, 490.0, 447.0, 296.0, 182.00738525390625, 208.0, 128.5, 182.00738525390625, 332.5, 490.0, 182.00738525390625, 320.0, 490.0, 205.5, 224.5, 470.0, 216.0, 462.5, 470.0, 182.00738525390625, 344.5, 290.0, 228.0, 224.0, 210.0, 462.5, 472.5, 180.0, 472.5, 182.00738525390625, 224.0, 490.0, 129.0, 296.0, 231.5, 348.5, 170.0, 226.5, 250.0, 128.5, 182.00738525390625, 129.0, 555.0, 490.0, 192.00738525390625, 290.0, 555.0, 180.00738525390625, 250.0, 490.0, 182.00738525390625, 190.0, 182.00738525390625, 348.5, 490.0, 189.50738525390625, 231.5, 223.0, 294.5, 224.0, 182.00738525390625, 228.5, 290.0, 182.00738525390625, 447.0, 182.00738525390625, 490.0, 250.0, 490.0, 447.0, 490.0, 290.0, 365.0, 490.0, 130.0, 472.5, 250.0, 296.0, 190.0, 290.0, 182.00738525390625, 205.5, 168.0, 521.5, 172.00738525390625, 182.00738525390625, 190.0, 182.00738525390625, 192.00738525390625, 224.0, 166.0, 250.0, 482.5, 386.0, 128.5, 336.5, 472.5, 129.5, 515.0, 575.0, 270.0, 190.0, 296.0] [99.5, 110.0, 50.0, 40.0, 99.5, 93.0, 50.0, 93.0, 99.5, 110.0, 14.0, 104.0, 105.0, 65.0, 110.0, 93.5, 99.5, 104.0, 105.0, 110.0, 104.0, 105.0, 90.0, 50.0, 105.0, 124.5, 110.0, 104.0, 110.0, 110.0, 110.0, 8.5, 104.0, 110.0, 107.0, 105.0, 104.0, 71.5, 104.0, 44.5, 110.0, 40.0, 104.0, 93.5, 110.0, 71.5, 106.0, 106.0, 99.5, 105.0, 99.5, 110.0, 99.5, 24.0, 105.0, 124.5, 114.0, 99.5, 8.5, 104.0, 65.0, 106.0, 93.5, 110.0, 93.5, 98.0, 110.0, 86.0, 104.0, 106.0, 73.0, 104.0, 105.0, 104.0, 107.0, 98.0, 104.0, 86.0, 105.0, 90.0, 104.0, 104.0, 99.5, 105.0, 99.5, 106.0, 74.0, 8.5, 110.0, 105.0, 104.0, 111.0, 106.0, 99.5, 110.0, 50.0, 114.0, 104.0, 104.0, 8.5, 110.0, 14.0, 126.0, 104.0, 124.5, 73.0, 111.0, 98.0, 98.0, 90.0, 105.0, 105.0, 93.5, 110.0, 105.0, 104.0, 75.0, 105.0, 110.0, 98.0, 110.0, 104.0, 99.5, 110.0, 104.0, 105.0, 99.5, 99.5, 104.0, 93.0, 105.0, 105.0, 104.0, 71.5, 110.0, 90.0, 99.5, 30.0, 99.5, 110.0, 104.0, 30.0, 99.5, 52.5, 104.0, 98.0, 110.0, 104.0, 104.0, 50.0, 98.0, 105.0, 110.0, 99.5]\n",
      "[100.0, 487.5, 47.0, 437.5, 816.5, 397.0, 86.5, 395.0, 1.5, 36.0, 1034.5, 115.0, 37.5, 145.0, 47.0, 195.5, 395.0, 291.5, 262.5, 395.0, 262.5, 2550.0, 197.5, 47.0, 387.5, 315.0, 227.5, 1063.5, 229.5, 255.0, 548.5, 395.0, 37.5, 1124.5, 291.5, 397.0, 252.0, 904.5, 393.0, 116.5, 106.5, 393.0, 116.5, 37.5, 154.0, 447.0, 4753.0, 262.5, 1.5, 164.0, 359.0, 55.0, 117.5, 65.5, 197.5, 264.5, 131.5, 548.5, 156.0, 156.0, 115.0, 1034.5, 47.5, 106.5, 588.0, 255.0, 598.5, 397.0, 4899.0, 47.0, 387.5, 816.5, 395.0, 267.5, 487.5, 115.0, 395.0, 41.0, 115.0, 30.5, 177.0, 38.5, 487.5, 373.0, 112.5, 132.0, 207.0, 4899.0, 262.5, 1324.5, 180.0, 487.5, 727.5, 395.0, 112.5, 393.0, 106.5, 393.0, 156.0, 32.5, 2605.0, 43.0, 195.5, 36.5, 259.0, 478.0, 171.0, 1173.0, 47.0, 29.0, 16.5, 164.0, 487.5, 275.5, 397.0, 680.5, 588.0, 74.5, 547.5, 4899.0, 1063.5, 252.0, 387.5, 164.0, 32.5, 157.5, 273.0, 283.5, 1.5, 115.0, 537.5, 1.5, 161.0, 487.5, 745.0, 154.0, 487.5, 395.0, 251.5, 282.5, 195.5, 157.5, 397.0, 395.0, 267.5, 395.0, 154.0, 246.5, 156.0, 816.5, 154.0, 395.0, 425.0, 816.5, 393.0, 148.5, 41.0, 23.5, 487.5, 19.5, 156.0, 3154.0, 235.0, 4899.0, 478.0, 125.0, 246.5, 1.5, 115.0, 164.0, 110.5, 57.5, 195.5, 115.0, 156.0, 229.5, 43.0, 197.5, 47.0, 32.5, 745.0, 1032.5, 229.5, 1.5, 487.5, 70.0, 457.0, 1.5, 1211.0, 784.5, 395.0, 53.0, 132.0, 36.0, 487.5, 195.5, 164.0, 200.5, 100.0, 1655.0, 282.5, 283.5, 110.5, 487.5, 164.0, 2550.0, 2148.5, 47.0, 3699.0, 4753.0, 273.0, 37.5, 159.0, 1.5, 280.0, 393.0, 393.0, 395.0, 47.0, 267.5, 148.5, 110.5, 745.0, 235.0, 1.5, 119.5, 264.5, 262.5, 2598.5, 147.5, 170.0, 47.0, 1063.5, 262.5, 1279.5, 229.5, 93.5, 63.5, 1086.0, 1148.5, 538.5, 419.0, 283.5, 544.0, 138.0, 3098.5, 1063.5, 487.5, 262.5, 1280.5, 222.0, 26.5, 487.5, 164.0, 156.0, 267.5, 114.0, 518.5, 47.0, 106.5, 478.0, 96.5, 283.5, 161.0, 4104.0, 2598.5, 148.5, 395.0, 291.5, 4354.0, 28.0, 154.0, 1280.5, 447.0, 478.0, 25.0, 548.5, 487.5, 4750.0, 397.0, 768.5, 262.5, 4899.0, 487.5, 1.5, 358.5, 3348.5, 487.5, 269.5, 42.0, 264.5, 36.0, 164.0, 38.0, 283.5, 487.5, 47.0, 39.5, 487.5, 85.0, 347.5, 141.0, 548.5, 478.0, 420.0, 437.5, 1034.5, 861.0, 4104.0, 229.5, 195.5, 152.5] [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\id126493\\Documents\\GitHub\\interpret_basics2\\forest_surveyor\\structures.py:261: RuntimeWarning: invalid value encountered in true_divide\n",
      "  np.histogram(lowers, lower_bins)[0]).round(5)\n"
     ]
    }
   ],
   "source": [
    "walked.discretize_paths(mydata.var_dict,\n",
    "                        bins=4, equal_counts=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " 'child_features',\n",
       " 'class_col',\n",
       " 'class_names',\n",
       " 'encoder',\n",
       " 'features',\n",
       " 'forest',\n",
       " 'forest_stats',\n",
       " 'forest_stats_by_label',\n",
       " 'forest_walk',\n",
       " 'full_survey',\n",
       " 'get_label',\n",
       " 'lower_features',\n",
       " 'n_features',\n",
       " 'prediction_model',\n",
       " 'root_features',\n",
       " 'structure',\n",
       " 'tree_structures']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(f_walker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=16,\n",
       "            max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=5, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False,\n",
       "            random_state=1609987677, splitter='best')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_walker.forest.estimators_[648]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
