{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prologue\n",
    "Prepare the notebook for inline plotting<br>\n",
    "Load required libraries<br>\n",
    "Create custom functions<br>\n",
    "Load and preprocess data<br>\n",
    "Train a random forest using previously optimized/tuned hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utility code in the associated file performs the following steps:\n",
      "defines function to print pretty confusion matrix: plot_confusion_matrix()\n",
      "defines a function to get the class code by label: get_class_code()\n",
      "defines a function to plot a tree inline: tree_to_code()\n",
      "defines a function to extract all the structural arrays of a tree: get_tree_structure()\n",
      "defines a function to extract a metrics dictionary from a random forest: explore_forest()\n",
      "defines a function to pass batches of data to explore_forest(), split by correct/incorrect prediction: batch_analyse_model()\n",
      "defines function to plot the mean path lengths from an object returned by explore_forest(): plot_mean_path_lengths()\n",
      "defines a function to map the path of an instance down a tree: tree_path()\n",
      "defines a function to map the path of an instance down a tree ensemble: forest_path()\n",
      "defines a function to find the majority predicted class from object returned by forest_path(): major_class_from_forest_paths()\n",
      "defines a function to convert a tree into a function: tree_to_code()\n",
      "defines a function to get list of all the paths for one instance out of a forest_paths object: get_paths()\n",
      "defines a function to get basic labels back from one hot encoded label names: decode onehot\n",
      "defines a function to get basic labels back from one hot encoded label names for all paths in a get_paths() object: decode onehot\n",
      "defines a function with an apriori based algorithm for finding frequent patterns from a list of paths: apriori()\n",
      "defines a function to sort a frequent pattern list returned by apriori()\n",
      "defines a class to accumulate freqent patterns into a rule set rule_accumulator\n",
      "defines a function to apply an accumulated rule to a data set: apply_rule()\n",
      "\n",
      "\n",
      "Data Set Information:\n",
      "Orignates from: https://www.lendingclub.com/info/download-data.action\n",
      "\n",
      "See also:\n",
      "https://www.kaggle.com/wordsforthewise/lending-club\n",
      "\n",
      "Prepared by Nate George: https://github.com/nateGeorge/preprocess_lending_club_data\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Dev\\Prog\\Anaconda3\\lib\\site-packages\\IPython\\core\\pylabtools.py:171: DtypeWarning: Columns (0,18,48,58,117,128,129,130,133,134,135,138,144,145,146) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  safe_execfile(fname,*where,**kw)\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "%matplotlib inline\n",
    "%run rf_analysis_utils.py\n",
    "%run lending_dataprep.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data objects are now saved and given a set of generic names ease of use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# get the correct directory for saved objects\n",
    "pickle_dir_store = open(\"pickle_dir.pickle\", \"rb\")\n",
    "pickle_dir = pickle.load(pickle_dir_store)\n",
    "pickle_dir_store.close()\n",
    "\n",
    "# helper function for pickling files\n",
    "def pickle_path(filename):\n",
    "    return(pickle_dir + '\\\\' + filename)\n",
    "\n",
    "# load up the training set (required because of running from script into Jup Note)\n",
    "encoder_store = open(pickle_path('encoder.pickle'), \"rb\")\n",
    "encoder = pickle.load(encoder_store)\n",
    "encoder_store.close()\n",
    "\n",
    "X_train_enc_store = open(pickle_path('X_train_enc.pickle'), \"rb\")\n",
    "X_train_enc = pickle.load(X_train_enc_store)\n",
    "X_train_enc_store.close()\n",
    "\n",
    "y_train_store = open(pickle_path('y_train.pickle'), \"rb\")\n",
    "y_train = pickle.load(y_train_store)\n",
    "y_train_store.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%run rf_analysis_modelprep.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# test the model on unseen data\n",
    "pred = enc_model.predict(X_test)\n",
    "print(\"Cohen's Kappa on unseen instances: \" \"{:0.4f}\".format(metrics.cohen_kappa_score(y_test, pred)))\n",
    "\n",
    "# view the confusion matrix\n",
    "cm = metrics.confusion_matrix(y_test, pred)\n",
    "plot_confusion_matrix(cm, classes=class_names,\n",
    "                      title='Confusion matrix, without normalization')\n",
    "plt.figure()\n",
    "# normalized confusion matrix\n",
    "plot_confusion_matrix(cm\n",
    "                      , classes=class_names\n",
    "                      , normalize=True,\n",
    "                      title='Normalized confusion matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1. Global Explanations\n",
    "## Statistical Analysis of a random forest model using data batches\n",
    "* Inductive Properties are based on the training data\n",
    "* Transductive Properties are based on the test (held out) data\n",
    "* In both cases, the whole dataset is passed into the model and statistics are gathered about how frequently the features are visited.\n",
    "\n",
    "* OOB data (to do)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# First instantiate new survey class with the RF model\n",
    "f_survey = forest_surveyor(model = rf, features = onehot_features, prediction_model = enc_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "########################################################################\n",
    "### This takes a few minutes, so only run when an update is required ###\n",
    "########################################################################\n",
    "\n",
    "if True:\n",
    "    \n",
    "    batch = X_test\n",
    "    labels = y_test\n",
    "\n",
    "    correct_preds = enc_model.predict(batch) == labels\n",
    "    incorrect_preds = enc_model.predict(batch) != labels\n",
    "    \n",
    "    if sum(correct_preds) > 0:\n",
    "        X = batch[correct_preds.values]\n",
    "        y = labels[correct_preds.values]\n",
    "        \n",
    "        f_survey.fit(encoder.transform(X), y, onehot_features)\n",
    "        f_cor_stats = f_survey.forest_stats(np.unique(y))\n",
    "        \n",
    "    if sum(incorrect_preds) > 0:\n",
    "        X = batch[incorrect_preds.values]\n",
    "        y = labels[incorrect_preds.values]\n",
    "        \n",
    "        f_survey.fit(encoder.transform(X), y, onehot_features)\n",
    "        f_incor_stats = f_survey.forest_stats(np.unique(y))\n",
    "    \n",
    "    tt_correct_stats_store = open(pickle_path('tt_correct_stats.pickle'), \"wb\")\n",
    "    pickle.dump(f_cor_stats, tt_correct_stats_store)\n",
    "    tt_correct_stats_store.close()\n",
    "    \n",
    "    tt_incorrect_stats_store = open(pickle_path('tt_incorrect_stats.pickle'), \"wb\")\n",
    "    pickle.dump(f_incor_stats, tt_incorrect_stats_store)\n",
    "    tt_incorrect_stats_store.close()\n",
    "    \n",
    "    \n",
    "tt_correct_stats_store = open(pickle_path('tt_correct_stats.pickle'), \"rb\")\n",
    "tt_correct_stats = pickle.load(tt_correct_stats_store)\n",
    "tt_correct_stats_store.close()\n",
    "\n",
    "tt_incorrect_stats_store = open(pickle_path('tt_incorrect_stats.pickle'), \"rb\")\n",
    "tt_incorrect_stats = pickle.load(tt_incorrect_stats_store)\n",
    "tt_incorrect_stats_store.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "log_ratio = log_ratio_plot(num = tt_correct_stats[0]['m_child_traversals']\n",
    "                            , num_err = tt_correct_stats[0]['se_child_traversals']\n",
    "                            , denom = tt_incorrect_stats[0]['m_child_traversals']\n",
    "                            , denom_err = tt_incorrect_stats[0]['se_child_traversals']\n",
    "                            , labels = onehot_features\n",
    ")\n",
    "log_ratio = log_ratio_plot(num = tt_correct_stats[0]['m_lower_traversals']\n",
    "                            , num_err = tt_correct_stats[0]['se_lower_traversals']\n",
    "                            , denom = tt_incorrect_stats[0]['m_lower_traversals']\n",
    "                            , denom_err = tt_incorrect_stats[0]['se_lower_traversals']\n",
    "                            , labels = onehot_features\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "log_ratio = log_ratio_plot(num = tt_correct_stats[1]['m_child_traversals'],\n",
    "                           denom = tt_correct_stats[0]['m_child_traversals'],\n",
    "                           num_err = tt_correct_stats[1]['sd_child_traversals'],\n",
    "                           denom_err = tt_correct_stats[0]['sd_child_traversals'],\n",
    "                           labels = onehot_features)\n",
    "\n",
    "log_ratio = log_ratio_plot(num = tt_correct_stats[1]['m_lower_traversals'],\n",
    "                           denom = tt_correct_stats[0]['m_lower_traversals'],\n",
    "                           num_err = tt_correct_stats[1]['sd_lower_traversals'],\n",
    "                           denom_err = tt_correct_stats[0]['sd_lower_traversals'],\n",
    "                           labels = onehot_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2. Local Explanations\n",
    "## Analysis of decision paths for individual unseen instances in a random forest model\n",
    "* Frequent Pattern Mining of decision paths\n",
    "* Rule Compression of decision paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "first_n = 14 # python is 'up to but not including'\n",
    "\n",
    "batch = X_test[0:first_n]\n",
    "labels = y_test[0:first_n]\n",
    "\n",
    "preds = enc_model.predict(batch)\n",
    "\n",
    "nfp = forest_path(forest = rf\n",
    "                , feature_names = onehot_features\n",
    "                , instances = batch\n",
    "                , labels = None\n",
    "                , feature_encoding = encoder\n",
    "                , by_tree = False)\n",
    "\n",
    "paths = {}\n",
    "for instance in range(first_n):\n",
    "    paths[instance] = get_paths(nfp, instance, by_tree = False, which_trees='majority')\n",
    "\n",
    "# there is a bug in sklearn causing all the warnings. This should be fixed in next release.\n",
    "pretty_print_tree_votes(paths, preds, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# profiling a single instance\n",
    "instance = 1\n",
    "\n",
    "# Collect useful data for instance\n",
    "true_class = labels[instance:instance+1].values[0]\n",
    "pred_probs = enc_model.predict_proba(batch[instance:instance + 1])[0]\n",
    "\n",
    "print('Test instance id: ' + str(instance))\n",
    "print('True class: ' + str(true_class))\n",
    "print('Pedicted Class Probabilities ')\n",
    "for c, n, p in zip(class_names, range(len(class_names)), pred_probs):\n",
    "    print(n, c, \"{:0.4f}\".format(p))\n",
    "\n",
    "# get the class codes produced by the model and the true class\n",
    "true_class_code = get_label(class_col, true_class)\n",
    "\n",
    "class_code_confs = np.argsort(pred_probs)[::-1] # descending order\n",
    "pred_class_code = get_code(class_col, class_names[class_code_confs[0]])\n",
    "second_class_code = get_code(class_col, class_names[class_code_confs[1]])\n",
    "\n",
    "# Display instance row\n",
    "germ_id = batch[instance:instance + 1].index[0]\n",
    "german[germ_id:germ_id + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# training data - could be a representative sample if original tr is too large or unavailable\n",
    "training_data = X_train\n",
    "training_labels = y_train\n",
    "\n",
    "# MAJORITY\n",
    "# generate a set of paths, sorting can be tuned\n",
    "sorted_fp = get_sorted_fp(discretize_paths(get_paths(nfp, instance, by_tree = False, which_trees='majority')\n",
    "                                           , vars_dict, 4),\n",
    "                                 support = 0.1, max_itemset_size = 6, alpha = 0.0)\n",
    "# create the rule accumulator\n",
    "ra = rule_accumulator(vars_dict=vars_dict, onehot_dict = onehot_dict, rule_list=sorted_fp)\n",
    "\n",
    "# run the profile\n",
    "ra, model_votes = profile_instance(instance=instance, target_class=0\n",
    "                                     , fp_object = nfp\n",
    "                                     , rule_acc = ra\n",
    "                                     , training_data = encoder.transform(training_data)\n",
    "                                     , training_labels = training_labels\n",
    "                                     , features = onehot_features\n",
    "                                     , class_names = class_names\n",
    "                                     , stopping_param = 1)\n",
    "\n",
    "# MINORITY\n",
    "# generate a set of paths, sorting can be tuned\n",
    "sorted_fp_min = get_sorted_fp(discretize_paths(get_paths(nfp, instance, by_tree = False, which_trees='minority')\n",
    "                                               , vars_dict, 4),\n",
    "                                 support = 0.1, max_itemset_size = 6, alpha = 0.0)\n",
    "\n",
    "# create the rule accumulator\n",
    "ra_min = rule_accumulator(vars_dict=vars_dict, onehot_dict = onehot_dict, rule_list=sorted_fp_min)\n",
    "\n",
    "# run the profile\n",
    "ra_min, model_votes = profile_instance(instance=instance, target_class=0\n",
    "                                     , fp_object = nfp\n",
    "                                     , rule_acc = ra_min\n",
    "                                     , training_data = encoder.transform(training_data)\n",
    "                                     , training_labels = training_labels\n",
    "                                     , features = onehot_features\n",
    "                                     , class_names = class_names\n",
    "                                     , stopping_param = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pretty_print(ra.rule, onehot_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ra.prune_rule()\n",
    "pretty_print(ra.pruned_rule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "p_count(training_labels.loc[apply_rule(ra.pruned_rule, encoder.transform(training_data), onehot_features)].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ra.pruned_rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "p_count(training_labels.loc[apply_rule([\n",
    " ('crhis_A30', True, 0.5),\n",
    " ('crhis_A31', True, 0.5),\n",
    " ('job_A174', False, 0.5),\n",
    " ('tel_A192', False, 0.5),\n",
    " ('emp_A75', False, 0.5),\n",
    " #('pers_A93', False, 0.5),\n",
    " ('chk_A12', False, 0.5)\n",
    "], encoder.transform(training_data), onehot_features)].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ra4 = rule_accumulator(vars_dict=vars_dict, onehot_dict = onehot_dict, rule_list=sorted_fp_min)\n",
    "for i in range(4):\n",
    "    ra4.add_rule()\n",
    "ra4.prune_rule()\n",
    "pretty_print(ra4.pruned_rule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "p_count(training_labels.loc[apply_rule(ra4.pruned_rule, encoder.transform(training_data), onehot_features)].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pretty_print(ra_min.rule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ra_min.prune_rule()\n",
    "pretty_print(ra_min.pruned_rule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "p_count(training_labels.loc[apply_rule(ra_min.pruned_rule, encoder.transform(training_data), onehot_features)].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pretty_print(ra_min.tight_rule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "p_count(training_labels.loc[apply_rule(ra_min.tight_rule, encoder.transform(training_data), onehot_features)].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3. Other Demos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## treeinterpreter - local explanations from conditional probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from treeinterpreter import treeinterpreter as ti, utils\n",
    "prediction, bias, contributions = ti.predict(rf, encoder.transform(batch[instance:instance + 1]))\n",
    "\n",
    "interp_cols = sum([['predictor'], class_names, ['abseff']], [])\n",
    "interp = pd.DataFrame(columns=interp_cols)\n",
    "# now extract contributions for each instance\n",
    "for c, feature in zip(contributions[0], onehot_features):\n",
    "    if any(c != 0):\n",
    "        vals = c.tolist()\n",
    "        vals.insert(0, feature)\n",
    "        vals.append(sum(abs(c)))\n",
    "        interp = interp.append(dict(zip(interp_cols, vals))\n",
    "                               , ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "interp = interp.sort_values('abseff', ascending=False).reset_index().drop('index', axis=1)\n",
    "interp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "interp = interp.sort_values('bad', ascending=False).reset_index().drop('index', axis=1)\n",
    "interp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "priors = dict(zip(interp_cols[1:],  np.append(bias[0], 1.0)))\n",
    "priors['predictor'] = 'priors (bias)'\n",
    "totals = dict(zip(interp_cols[1:],  interp.sum()[1:].values))\n",
    "totals['predictor'] = 'Total'\n",
    "preds = dict(zip(interp_cols[1:],  np.append(prediction, 1.0)))\n",
    "preds['predictor'] = 'P(class)'\n",
    "interp_totals = pd.DataFrame(columns=interp_cols)\n",
    "interp_totals = interp_totals.append(priors, ignore_index=True)\n",
    "\n",
    "interp_totals = interp_totals.append(totals, ignore_index=True)\n",
    "interp_totals = interp_totals.append(preds, ignore_index=True)\n",
    "interp_totals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# basic setup\n",
    "import lime\n",
    "import lime.lime_tabular as limtab\n",
    "\n",
    "np.random.seed(seed)\n",
    "\n",
    "explainer = limtab.LimeTabularExplainer(training_data=np.array(X_train_enc.todense())\n",
    "                                        , feature_names=onehot_features\n",
    "                                        , training_labels=y_train\n",
    "                                        , class_names=class_names\n",
    "                                        , categorical_features=range(len(onehot_features))\n",
    "                                        , categorical_names=onehot_features\n",
    "                                        , mode='classification'\n",
    "                                        , discretize_continuous=False\n",
    "                                        , verbose=False)\n",
    "\n",
    "exp = explainer.explain_instance(np.array(encoder.transform(batch[instance:instance+1]).todense())[0]\n",
    "                                 , rf.predict_proba\n",
    "                                 , top_labels=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = exp.as_pyplot_figure(label=pred_class_code)\n",
    "\n",
    "exp.as_list(label=pred_class_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Show the runner up class details\n",
    "fig = exp.as_pyplot_figure(label=second_class_code)\n",
    "exp.as_list(label=second_class_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "exp.show_in_notebook()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
